{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c1317b-56e1-4614-a978-100dd404d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import expr\n",
    "\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType,DatetimeConverter,DateType,DoubleType,TimestampType,FloatType\n",
    "# import json\n",
    "# from time import sleep\n",
    "# import concurrent.futures\n",
    "# # Create a Spark session\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName('KafkaStreamJoinExample') \\\n",
    "#     .getOrCreate()# <---- app name\n",
    "\n",
    "# kafka_bootstrap_servers = 'localhost:9092'\n",
    "# olist_orders1 = 'topic1'\n",
    "# olist_customer2 = 'topic2'\n",
    "# olist_order_items3 = 'seller1'\n",
    "# olist_order_payments4 = 'newer'\n",
    "# olist_order_reviews5 = 'topic5'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "# #     # Submit tasks to create topics concurrently\n",
    "# #     # Submit tasks to run producers concurrently\n",
    "# #     futures = [executor.submit(run_producer, file, topic, producer) for file, topic in zip(files, kafka_topics)]\n",
    "# #     concurrent.futures.wait(futures)\n",
    "\n",
    "# # producer.close()\n",
    "\n",
    "\n",
    "# # Read from Kafka topic 1\n",
    "# olist_orders = spark.readStream \\\n",
    "#     .format('kafka') \\\n",
    "#     .option('kafka.bootstrap.servers', kafka_bootstrap_servers) \\\n",
    "#     .option('subscribe', olist_order_items3) \\\n",
    "#     .load() \\\n",
    "#     .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "#     .select(from_json(col(\"value\"), csv_schema3).alias(\"data\")) \\\n",
    "#     .select(\"data.*\")\n",
    "\n",
    "# olist_customer = spark.readStream \\\n",
    "#     .format('kafka') \\\n",
    "#     .option('kafka.bootstrap.servers', kafka_bootstrap_servers) \\\n",
    "#     .option('subscribe', olist_order_payments4) \\\n",
    "#     .load() \\\n",
    "#     .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "#     .select(from_json(col(\"value\"), csv_schema4).alias(\"data\")) \\\n",
    "#     .select(\"data.*\")\n",
    "\n",
    "# processed_df = olist_orders.select(\"product_id\", \"price\")\n",
    "\n",
    "# # Start the streaming query\n",
    "# query1 = processed_df.writeStream \\\n",
    "#     .outputMode('append') \\\n",
    "#     .format('console') \\\n",
    "#     .start()\n",
    "\n",
    "# processed_df1 = olist_customer.select(\"payment_type\", \"payment_installments\")\n",
    "\n",
    "# # Start the streaming query\n",
    "# query2 = processed_df.writeStream \\\n",
    "#     .outputMode('append') \\\n",
    "#     .format('console') \\\n",
    "#     .start()\n",
    "\n",
    "# query1.awaitTermination()\n",
    "\n",
    "# query2.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d8c295b-9867-4bf5-be0f-5b367c2ec9cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-04b207b14e92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'time'"
     ]
    }
   ],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col,expr,\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType,DatetimeConverter,DateType,DoubleType,TimestampType,FloatType\n",
    "import json\n",
    "from time import sleep\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e738b538-4890-4795-8912-2d1807795891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('KafkaConsumerVisualization') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb8805-cae9-4b38-a4f3-93b4ced1525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5\") \n",
    "\n",
    "# spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.11-2.4.5\") \n",
    "# spark-sql-kafka-0-10_2.12-3.1.1\n",
    "# org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5\n",
    "# org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\n",
    "# Kafka configuration\n",
    "kafka_bootstrap_servers = '127.0.0.1:9092'\n",
    "dash_topic='dash_topic'\n",
    "kafka_topics = ['orders','customers','order_items', 'order_payments','order_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b346074-24f6-419b-98f3-7d0a6548ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - olist_orders\n",
    "#  0   order_id                       object        \n",
    "#  1   customer_id                    object        \n",
    "#  2   order_status                   object        \n",
    "#  3   order_purchase_timestamp       datetime64[ns]\n",
    "#  4   order_approved_at              datetime64[ns]\n",
    "#  5   order_delivered_carrier_date   datetime64[ns]\n",
    "#  6   order_delivered_customer_date  datetime64[ns]\n",
    "#  7   order_estimated_delivery_date  datetime64[ns]\n",
    "csv_schema1 = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"order_purchase_timestamp\", TimestampType(), True),\n",
    "    StructField(\"order_approved_at\", TimestampType(), True),\n",
    "    StructField(\"order_delivered_carrier_date\", TimestampType(), True),\n",
    "    StructField(\"order_delivered_customer_date\", TimestampType(), True),\n",
    "    StructField(\"order_estimated_delivery_date\", TimestampType(), True),\n",
    "    StructField(\"current_time\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# - olist_customer\n",
    "#  0   customer_id               object\n",
    "#  1   customer_unique_id        object\n",
    "#  2   customer_zip_code_prefix  int64 \n",
    "#  3   customer_city             object\n",
    "#  4   customer_state            object\n",
    "csv_schema2 = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_unique_id\", StringType(), True),\n",
    "    StructField(\"customer_zip_code_prefix\", IntegerType(), True),\n",
    "    StructField(\"customer_city\", StringType(), True),\n",
    "    StructField(\"customer_state\", StringType(), True),\n",
    "    StructField(\"current_time\", TimestampType(), True)\n",
    "    \n",
    "])\n",
    "\n",
    "# - olist_order_items\n",
    "# 0   order_id             object        \n",
    "# 1   order_item_id        int64         \n",
    "# 2   product_id           object        \n",
    "# 3   seller_id            object        \n",
    "# 4   shipping_limit_date  datetime64[ns]\n",
    "# 5   price                float64       \n",
    "# 6   freight_value        float64  \n",
    "csv_schema3 = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"order_item_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"seller_id\", StringType(), True),\n",
    "    StructField(\"shipping_limit_date\", TimestampType(), True),\n",
    "    StructField(\"price\", FloatType(), True),\n",
    "    StructField(\"freight_value\", FloatType(), True),\n",
    "    StructField(\"current_time\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# - olist_order_payments\n",
    "# 0   order_id              103886 non-null  object \n",
    "# 1   payment_sequential    103886 non-null  int64  \n",
    "# 2   payment_type          103886 non-null  object \n",
    "# 3   payment_installments  103886 non-null  int64  \n",
    "# 4   payment_value         103886 non-null  float64\n",
    "csv_schema4 = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"payment_sequential\", IntegerType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"payment_installments\", IntegerType(), True),\n",
    "    StructField(\"payment_value\", FloatType(), True),\n",
    "    StructField(\"current_time\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# - olist_order_reviews\n",
    "# 0   review_id                99224 non-null  object\n",
    "# 1   order_id                 99224 non-null  object\n",
    "# 2   review_score             99224 non-null  int64 \n",
    "# 3   review_comment_title     11568 non-null  object\n",
    "# 4   review_comment_message   40977 non-null  object\n",
    "# 5   review_creation_date     99224 non-null  object\n",
    "# 6   review_answer_timestamp  99224 non-null  object\n",
    "csv_schema5 = StructType([\n",
    "    StructField(\"review_id\", StringType(), True),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"review_score\", IntegerType(), True),\n",
    "    StructField(\"review_creation_date\", StringType(), True),\n",
    "    StructField(\"review_answer_timestamp\", StringType(), True),\n",
    "    StructField(\"current_time\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c2e5d5-634e-419f-8de7-a7105e1f63f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sellers\n",
    "# 0   seller_id               3095 non-null   object\n",
    "# 1   seller_zip_code_prefix  3095 non-null   int64 \n",
    "# 2   seller_city             3095 non-null   object\n",
    "# 3   seller_state            3095 non-null   object\n",
    "sellers = StructType([\n",
    "    StructField(\"seller_id\", StringType(), True),\n",
    "    StructField(\"seller_zip_code_prefix\", IntegerType(), True),\n",
    "    StructField(\"seller_city\", StringType(), True),\n",
    "    StructField(\"seller_state\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# products\n",
    "# 0   product_id                  32951 non-null  object \n",
    "# 1   product_category_name       32341 non-null  object \n",
    "# 2   product_name_lenght         32341 non-null  float64\n",
    "# 3   product_description_lenght  32341 non-null  float64\n",
    "# 4   product_photos_qty          32341 non-null  float64\n",
    "# 5   product_weight_g            32949 non-null  float64\n",
    "# 6   product_length_cm           32949 non-null  float64\n",
    "# 7   product_height_cm           32949 non-null  float64\n",
    "# 8   product_width_cm            32949 non-null  float64\n",
    "\n",
    "products = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_category_name\", StringType(), True),\n",
    "    StructField(\"product_name_lenght\", FloatType(), True),\n",
    "    StructField(\"product_description_lenght\", FloatType(), True),\n",
    "    StructField(\"product_photos_qty\", FloatType(), True),\n",
    "    StructField(\"product_weight_g\", FloatType(), True),\n",
    "    StructField(\"product_length_cm\", FloatType(), True),\n",
    "    StructField(\"product_height_cm\", FloatType(), True),\n",
    "    StructField(\"product_width_cm\", FloatType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7e98e-f577-4d11-bc6c-cd340ad91bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from Kafka using Structured Streaming\n",
    "orders = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', '127.0.0.1:9092') \\\n",
    "    .option(\"subscribe\", kafka_topics[0]) \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), csv_schema1).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "customers = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', '127.0.0.1:9092') \\\n",
    "    .option(\"subscribe\", kafka_topics[1]) \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), csv_schema2).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "order_items = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', '127.0.0.1:9092') \\\n",
    "    .option(\"subscribe\", kafka_topics[2]) \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), csv_schema3).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "order_payments = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', '127.0.0.1:9092') \\\n",
    "    .option(\"subscribe\", kafka_topics[3]) \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), csv_schema4).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "\n",
    "order_reviews = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', '127.0.0.1:9092') \\\n",
    "    .option(\"subscribe\", kafka_topics[4]) \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), csv_schema5).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e718d-0c37-4655-aea7-c741edf72f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining ------------------------------------------------------------------------------------------------------------------\n",
    "#static data using parquet\n",
    "olist_products = spark.read.parquet(\"products.parquet\")\n",
    "olist_categname = spark.read.parquet(\"categname.parquet\")\n",
    "olist_sellers = spark.read.parquet(\"sellers.parquet\")\n",
    "\n",
    "\n",
    "#realtime data\n",
    "# Windowing by timestamp with 1-minute intervals\n",
    "from pyspark.sql.functions import timestamp_seconds\n",
    "\n",
    "\n",
    "sdf.select(\n",
    "   'name',\n",
    "   timestamp_seconds(sdf.time).alias('time') #we need to add this after every 'data.*' in evert last select of df\n",
    "                    |\n",
    "                    |\n",
    ").withWatermark('time', '10 minutes')\n",
    "                    |        \n",
    "                    +-----------------------+  # can do it like this if time stamp column is not present          \n",
    "                                            |\n",
    "sensor_window = sensor_df.withWatermark(\"current_time\", \"1 minute\")\\\n",
    "  .groupBy(\"sensor_id\").window(\"1 minute\").agg(avg(\"value\")) # we don't need groupby just simple join\n",
    "\n",
    "user_window = user_df.withWatermark(\"timestamp\", \"1 minute\")\\\n",
    "  .groupBy(\"user_id\").window(\"1 minute\").agg(count(\"action\")) # we don't need groupby just simple join\n",
    "\n",
    "\n",
    "joined_df = sensor_window.join(user_window, on=[\"timestamp\", \"user_id\"], how=\"outer\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fde524-fe9f-4069-9213-cd5a69f00ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToDash(writeDF, _):\n",
    "    writeDF.writeStream \\\n",
    "        .format('kafka') \\\n",
    "        .option('kafka.bootstrap.servers', '127.0.0.1:9092') \\\n",
    "        .option('topic', 'dash_topic') \\\n",
    "        .start()\n",
    "    #.option('checkpointLocation', '/hdfs') \\\n",
    "    \n",
    "\n",
    "def writeToHive1(writeDF, _):\n",
    "    writeDF.write \\\n",
    "        .mode('append') \\\n",
    "        .saveAsTable('ml_table')\n",
    "\n",
    "join_5.writeStream \\\n",
    "    .option(\"spark.cassandra.connection.host\",\"127.0.0.1:9092\")\\\n",
    "    .foreachBatch(writeToDash) \\\n",
    "    .foreachBatch(writeToHive1) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\\\n",
    "    .awaitTermination()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# add\n",
    "# we need to add this after every 'data.*' in evert last select of df\n",
    "\n",
    "b_orders = orders.withWatermark(\"current_time\", \"8 seconds\")\n",
    "b_customers = customers.withWatermark(\"current_time\", \"8 seconds\")\n",
    "b_order_items = order_items.withWatermark(\"current_time\", \"8 seconds\")\n",
    "b_order_payments = order_payments.withWatermark(\"current_time\", \"8 second\")\n",
    "b_order_reviews = order_reviews.withWatermark(\"current_time\", \"8 seconds\")\n",
    "\n",
    "\n",
    "join_1c=[\n",
    "b_orders['customer_id']==b_customers['customer_id'],\n",
    "b_orders['current_time']>=b_customers['current_time'],\n",
    "b_orders['current_time']<=b_customers['current_time']+expr('interval 8 seconds')   \n",
    "]\n",
    "\n",
    "\n",
    "join_2c=[\n",
    "join_1['order_id']==b_order_items['order_id'],\n",
    "join_1['current_time']>=b_order_items['current_time'],\n",
    "join_1['current_time']<=b_order_items['current_time']+expr('interval 8 seconds')   \n",
    "]\n",
    "\n",
    "join_3c=[\n",
    "join_2['order_id']==b_order_payments['order_id'],\n",
    "join_2['current_time']>=b_order_payments['current_time'],\n",
    "join_2['current_time']<=b_order_payments['current_time']+expr('interval 8 seconds')   \n",
    "]\n",
    "\n",
    "join_4c=[\n",
    "join_3['order_id']==b_order_reviews['order_id'],\n",
    "join_3['current_time']>=b_order_reviews['current_time'],\n",
    "join_3['current_time']<=b_order_reviews['current_time']+expr('interval 8 seconds')   \n",
    "]\n",
    "\n",
    "join_1=b_orders.join(b_customers,join_1c)\n",
    "join_2=join_1.join(b_order_items,join_2c)\n",
    "join_3=join_2.join(b_order_payments,join_3c)\n",
    "join_4=join_3.join(b_order_reviews,join_4c)\n",
    "\n",
    "\n",
    "join_5=join_4.join(olist_products,'product_id').join(olist_categname,'product_category_name').join(olist_sellers,'seller_id')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data = olist_orders.merge(olist_customer, on=\"customer_id\") \\\n",
    "# .merge(olist_order_items, on=\"order_id\") \\\n",
    "# .merge(olist_products , on=\"product_id\") \\\n",
    "# .merge(olist_categname, on=\"product_category_name\") \\\n",
    "# .merge(olist_order_payments, on=\"order_id\") \\\n",
    "# .merge(olist_sellers, on=\"seller_id\") \\\n",
    "# .merge(olist_order_reviews, on=\"order_id\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Parse the JSON data based on the defined schema\n",
    "# df_parsed = df.select(from_json(col(\"value\"), csv_schema3).alias(\"data\")) \\\n",
    "#     .select(\"data.*\")\n",
    "\n",
    "\n",
    "# # this function will feed the real time dash board\n",
    "# def writeToDash(writeDF, _):\n",
    "#     writeDF.writeStream \\\n",
    "#         .format('kafka') \\\n",
    "#         .option('kafka.bootstrap.servers', '127.0.0.1:9092') \\\n",
    "#         .option('topic', 'dash_topic') \\\n",
    "#         .start()\n",
    "#     #.option('checkpointLocation', '/hdfs') \\\n",
    "    \n",
    "\n",
    "# def writeToHive1(writeDF, _):\n",
    "#     writeDF.write \\\n",
    "#         .mode('append') \\\n",
    "#         .saveAsTable('ml_table')\n",
    "\n",
    "# df1.writeStream \\\n",
    "#     .option(\"spark.cassandra.connection.host\",\"127.0.0.1:9092\")\\\n",
    "#     .foreachBatch(writeToDash) \\\n",
    "#     .outputMode(\"update\") \\\n",
    "#     .start()\\\n",
    "#     .awaitTermination()\n",
    "    # it will direct all batch to streamlit\n",
    "    # .foreachBatch(writeToHive2) \\ # to save it in hive\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Display the streaming data in Databricks\n",
    "# try:\n",
    "#     # This works in a Databricks notebook environment\n",
    "#     df_parsed.writeStream \\\n",
    "#     .outputMode('append') \\\n",
    "#     .format('console') \\\n",
    "#     .start()\n",
    "# except NameError:\n",
    "#     pass  # Ignore if not running in Databricks\n",
    "\n",
    "# Await termination (this will only be reached when the streaming query is manually stopped)\n",
    "# query.awaitTermination()\n",
    "\n",
    "# Stop the Spark session\n",
    "# spark.stop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#spark = SparkSession \\\n",
    "#    .builder \\\n",
    "#    .appName(\"Streaming from Kafka\") \\\n",
    "#    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "#    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.11-2.4.5') \\\n",
    "#    .master(\"local[2]\") \\\n",
    "#    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193568ba-7ce7-4a88-9bca-a1d6a725cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col,expr,\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType,DatetimeConverter,DateType,DoubleType,TimestampType,FloatType\n",
    "import json\n",
    "from time import sleep\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('KafkaConsumerVisualization') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "kafka_bootstrap_servers = '127.0.0.1:9092'\n",
    "dash_topic='dash_topic'\n",
    "kafka_topics = ['orders','customers','order_items', 'order_payments','order_reviews']\n",
    "\n",
    "\n",
    "# - olist_orders\n",
    "#  0   order_id                       object        \n",
    "#  1   customer_id                    object        \n",
    "#  2   order_status                   object        \n",
    "#  3   order_purchase_timestamp       datetime64[ns]\n",
    "#  4   order_approved_at              datetime64[ns]\n",
    "#  5   order_delivered_carrier_date   datetime64[ns]\n",
    "#  6   order_delivered_customer_date  datetime64[ns]\n",
    "#  7   order_estimated_delivery_date  datetime64[ns]\n",
    "csv_schema1 = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"order_purchase_timestamp\", TimestampType(), True),\n",
    "    StructField(\"order_approved_at\", TimestampType(), True),\n",
    "    StructField(\"order_delivered_carrier_date\", TimestampType(), True),\n",
    "    StructField(\"order_delivered_customer_date\", TimestampType(), True),\n",
    "    StructField(\"order_estimated_delivery_date\", TimestampType(), True),\n",
    "    StructField(\"current_time\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# - olist_customer\n",
    "#  0   customer_id               object\n",
    "#  1   customer_unique_id        object\n",
    "#  2   customer_zip_code_prefix  int64 \n",
    "#  3   customer_city             object\n",
    "#  4   customer_state            object\n",
    "csv_schema2 = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_unique_id\", StringType(), True),\n",
    "    StructField(\"customer_zip_code_prefix\", IntegerType(), True),\n",
    "    StructField(\"customer_city\", StringType(), True),\n",
    "    StructField(\"customer_state\", StringType(), True),\n",
    "    StructField(\"current_time\", TimestampType(), True)\n",
    "    \n",
    "])\n",
    "\n",
    "# - olist_order_items\n",
    "# 0   order_id             object        \n",
    "# 1   order_item_id        int64         \n",
    "# 2   product_id           object        \n",
    "# 3   seller_id            object        \n",
    "# 4   shipping_limit_date  datetime64[ns]\n",
    "# 5   price                float64       \n",
    "# 6   freight_value        float64  \n",
    "csv_schema3 = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"order_item_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"seller_id\", StringType(), True),\n",
    "    StructField(\"shipping_limit_date\", TimestampType(), True),\n",
    "    StructField(\"price\", FloatType(), True),\n",
    "    StructField(\"freight_value\", FloatType(), True),\n",
    "    StructField(\"current_time\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# - olist_order_payments\n",
    "# 0   order_id              103886 non-null  object \n",
    "# 1   payment_sequential    103886 non-null  int64  \n",
    "# 2   payment_type          103886 non-null  object \n",
    "# 3   payment_installments  103886 non-null  int64  \n",
    "# 4   payment_value         103886 non-null  float64\n",
    "csv_schema4 = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"payment_sequential\", IntegerType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"payment_installments\", IntegerType(), True),\n",
    "    StructField(\"payment_value\", FloatType(), True),\n",
    "    StructField(\"current_time\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# - olist_order_reviews\n",
    "# 0   review_id                99224 non-null  object\n",
    "# 1   order_id                 99224 non-null  object\n",
    "# 2   review_score             99224 non-null  int64 \n",
    "# 3   review_comment_title     11568 non-null  object\n",
    "# 4   review_comment_message   40977 non-null  object\n",
    "# 5   review_creation_date     99224 non-null  object\n",
    "# 6   review_answer_timestamp  99224 non-null  object\n",
    "csv_schema5 = StructType([\n",
    "    StructField(\"review_id\", StringType(), True),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"review_score\", IntegerType(), True),\n",
    "    StructField(\"review_creation_date\", StringType(), True),\n",
    "    StructField(\"review_answer_timestamp\", StringType(), True),\n",
    "    StructField(\"current_time\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Read data from Kafka using Structured Streaming\n",
    "orders = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', '127.0.0.1:9092') \\\n",
    "    .option(\"subscribe\", kafka_topics[0]) \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), csv_schema1).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "customers = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', '127.0.0.1:9092') \\\n",
    "    .option(\"subscribe\", kafka_topics[1]) \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), csv_schema2).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "order_items = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', '127.0.0.1:9092') \\\n",
    "    .option(\"subscribe\", kafka_topics[2]) \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), csv_schema3).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "order_payments = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', '127.0.0.1:9092') \\\n",
    "    .option(\"subscribe\", kafka_topics[3]) \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), csv_schema4).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "\n",
    "order_reviews = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', '127.0.0.1:9092') \\\n",
    "    .option(\"subscribe\", kafka_topics[4]) \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), csv_schema5).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "olist_products = spark.read.parquet(\"products.parquet\") #<-- to create throught spark to parquet to hdfs\n",
    "olist_categname = spark.read.parquet(\"categname.parquet\") #<-- to create\n",
    "olist_sellers = spark.read.parquet(\"sellers.parquet\")#<-- to create\n",
    "\n",
    "\n",
    "b_orders = orders.withWatermark(\"current_time\", \"8 seconds\")\n",
    "b_customers = customers.withWatermark(\"current_time\", \"8 seconds\")\n",
    "b_order_items = order_items.withWatermark(\"current_time\", \"8 seconds\")\n",
    "b_order_payments = order_payments.withWatermark(\"current_time\", \"8 second\")\n",
    "b_order_reviews = order_reviews.withWatermark(\"current_time\", \"8 seconds\")\n",
    "\n",
    "\n",
    "join_1c=[\n",
    "b_orders['customer_id']==b_customers['customer_id'],\n",
    "b_orders['current_time']>=b_customers['current_time'],\n",
    "b_orders['current_time']<=b_customers['current_time']+expr('interval 8 seconds')   \n",
    "]\n",
    "\n",
    "\n",
    "join_2c=[\n",
    "join_1['order_id']==b_order_items['order_id'],\n",
    "join_1['current_time']>=b_order_items['current_time'],\n",
    "join_1['current_time']<=b_order_items['current_time']+expr('interval 8 seconds')   \n",
    "]\n",
    "\n",
    "join_3c=[\n",
    "join_2['order_id']==b_order_payments['order_id'],\n",
    "join_2['current_time']>=b_order_payments['current_time'],\n",
    "join_2['current_time']<=b_order_payments['current_time']+expr('interval 8 seconds')   \n",
    "]\n",
    "\n",
    "join_4c=[\n",
    "join_3['order_id']==b_order_reviews['order_id'],\n",
    "join_3['current_time']>=b_order_reviews['current_time'],\n",
    "join_3['current_time']<=b_order_reviews['current_time']+expr('interval 8 seconds')   \n",
    "]\n",
    "\n",
    "join_1=b_orders.join(b_customers,join_1c)\n",
    "join_2=join_1.join(b_order_items,join_2c)\n",
    "join_3=join_2.join(b_order_payments,join_3c)\n",
    "join_4=join_3.join(b_order_reviews,join_4c)\n",
    "\n",
    "\n",
    "join_5=join_4.join(olist_products,'product_id').join(olist_categname,'product_category_name').join(olist_sellers,'seller_id')\n",
    "\n",
    "\n",
    "\n",
    "def writeToDash(writeDF, _):\n",
    "    writeDF.writeStream \\\n",
    "        .format('kafka') \\\n",
    "        .option('kafka.bootstrap.servers', '127.0.0.1:9092') \\\n",
    "        .option('topic', 'dash_topic') \\\n",
    "        .start()\n",
    "    #.option('checkpointLocation', '/hdfs') \\\n",
    "    \n",
    "\n",
    "def writeToHive1(writeDF, _):\n",
    "    writeDF.write \\\n",
    "        .mode('append') \\\n",
    "        .saveAsTable('ml_table')\n",
    "\n",
    "join_5.writeStream \\\n",
    "    .option(\"spark.cassandra.connection.host\",\"127.0.0.1:9092\")\\\n",
    "    .foreachBatch(writeToDash) \\\n",
    "    .foreachBatch(writeToHive1) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
